{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c119197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9279da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variable\n",
    "NUM_WORDS = 1000\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "PADDING = 'post'\n",
    "MAXLEN = 120\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e85804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve DS from Github\n",
    "dataset_link = 'https://raw.githubusercontent.com/Willie29/capstone-C23-PS056/main/ML/Dataset_Combined.csv'\n",
    "response = requests.get(dataset_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acceb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label                                              Tweet\n",
      "0  Non_HS  Fadli Zon Minta Mendagri Segera Menonaktifkan ...\n",
      "1  Non_HS  Mereka terus melukai aksi dalam rangka memenja...\n",
      "2  Non_HS  bagaimana gurbernur melakukan kekerasan peremp...\n",
      "3  Non_HS  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
      "4  Non_HS                Waspada KTP palsu.....kawal PILKADA\n",
      "\n",
      "\n",
      "     Label                                       Tweet\n",
      "2703    HS                          Dasar murahan kamu\n",
      "2704    HS     kuliah aja tinggi, tapi otak di dengkul\n",
      "2705    HS                    semoga anda masuk neraka\n",
      "2706    HS                               Matilo anjing\n",
      "2707    HS  Orang timur kurang pintar dari orang barat\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/Willie29/capstone-C23-PS056/main/ML/Dataset_Combined.csv')\n",
    "# Identify Unnamed columns\n",
    "unnamed_columns = [col for col in data.columns if 'Unnamed' in col]\n",
    "\n",
    "# Drop Unnamed columns\n",
    "data = data.drop(unnamed_columns, axis=1)\n",
    "\n",
    "print(data.head())\n",
    "print(\"\\n\")\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5816207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non_HS    1354\n",
       "HS        1354\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02874de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data):\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        cond = (0 if row['Label'] == \"HS\" else 1)\n",
    "        labels.append(cond)\n",
    "        tweets.append(row['Tweet'])\n",
    "            \n",
    "    return labels, tweets\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "labels, tweets = parse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00071549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example number in dataset is 2708 examples\n",
      "\n",
      "2nd example:\n",
      "Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.\n",
      "\n",
      "Last example:\n",
      "Orang timur kurang pintar dari orang barat\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example number in dataset is {len(tweets)} examples\\n\")\n",
    "\n",
    "print(f\"2nd example:\\n{tweets[1]}\\n\")\n",
    "print(f\"Last example:\\n{tweets[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7de9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#90-10 ratio train-test\n",
    "def train_test_split(labels,tweets):\n",
    "    train_size = int(len(tweets) * 0.9)\n",
    "\n",
    "    train_labels = labels[:train_size]\n",
    "    train_tweets = tweets[:train_size]\n",
    "\n",
    "    test_labels = labels[train_size:]\n",
    "    test_tweets = tweets[train_size:]\n",
    "    \n",
    "    return train_labels, train_tweets, test_labels, test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c25d91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2437 sentences for training.\n",
      " 2437 labels for training.\n",
      " 271 sentences for validation.\n",
      " 271 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "train_labels, train_tweets, test_labels, test_tweets = train_test_split(labels, tweets)\n",
    "\n",
    "print(f\" {len(train_labels)} sentences for training.\")\n",
    "print(f\" {len(train_tweets)} labels for training.\")\n",
    "print(f\" {len(test_labels)} sentences for validation.\")\n",
    "print(f\" {len(test_tweets)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "572a3819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fadli Zon Minta Mendagri Segera Menonaktifkan Ahok Jadi Gubernur DKI', 'Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.', 'bagaimana gurbernur melakukan kekerasan perempuan? Buktinya banyak ibu2 mau foto bareng #DebatFinalPilkadaJKT']\n",
      "[1, 1, 1]\n",
      "['gara gara ini negara kita diketawain negara sebelah aduuuh emang iq yang buat tinggi2 yaa ngga kuat gue', 'kasian para bapak bangsa yang merumuskan pancasila mereka pasti sedih klo tau ternyata generasi pancasila nya spt itu', 'mereka gak kaya seperti lu yang gak pernah susah dapat makan']\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[:3])\n",
    "print(train_labels[:3])\n",
    "print(test_tweets[:3])\n",
    "print(test_labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abe0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer function\n",
    "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b642bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat test doang\n",
    "def lowercase(list_sentence):\n",
    "    lower_sentence = list_sentence\n",
    "    for i in range(len(list_sentence)):\n",
    "        lower_sentence[i] = lower_sentence[i].lower()\n",
    "    return lower_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c41065c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 7765 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "#tokenize sentence\n",
    "#test_tweets1 = test_tweets[0].lower()\n",
    "#test_tweets1 = test_tweets1.lower()\n",
    "lower_train_tweets = lowercase(train_tweets)\n",
    "tokenizer = fit_tokenizer(lower_train_tweets, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abca6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq and padding function\n",
    "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen = maxlen, padding = padding)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c77efe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (2437, 120)\n",
      "\n",
      "Padded validation sequences have shape: (271, 120)\n"
     ]
    }
   ],
   "source": [
    "#seq and padding sentence\n",
    "lower_val_tweets = lowercase(test_tweets)\n",
    "train_padded_seq = seq_and_pad(lower_train_tweets, tokenizer, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(lower_val_tweets, tokenizer, PADDING, MAXLEN)\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b170d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d86bf14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Structure\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length = MAXLEN),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units = 32, activation= 'relu'),\n",
    "    tf.keras.layers.Dense(units = 2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "612ec45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "263d3867",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Compile Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam() ,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_padded_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_padded_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rovar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rovar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:985\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    982\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    984\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    988\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[0;32m    989\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    990\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    994\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "#Compile Model\n",
    "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam() ,metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_padded_seq, train_labels, epochs=30, validation_data=(val_padded_seq, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gak jalan gara gara labelnya list bukan numpy array, jadi antara di tokenize jadi otomatis keubah atau diubah ke np array secara manual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bed3b92dedaca74de8f0a13833bbbd67bbdaaa544f0c2aa325c872f95f3ec5b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
