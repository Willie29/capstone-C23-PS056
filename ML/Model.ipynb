{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5lR0uvOoiCSs"
      },
      "outputs": [],
      "source": [
        "#Import Library\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#global variable\n",
        "NUM_WORDS = 1000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "PADDING = 'post'\n",
        "MAXLEN = 120\n",
        "EMBEDDING_DIM = 16"
      ],
      "metadata": {
        "id": "rllPSi38iOzC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieve DS from Github\n",
        "dataset_link = 'https://raw.githubusercontent.com/Willie29/capstone-C23-PS056/main/ML/Dataset_Combined.csv'\n",
        "response = requests.get(dataset_link)"
      ],
      "metadata": {
        "id": "65qUSIjFiVZ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/Willie29/capstone-C23-PS056/main/ML/Dataset_Combined.csv')\n",
        "# Identify Unnamed columns\n",
        "unnamed_columns = [col for col in data.columns if 'Unnamed' in col]\n",
        "\n",
        "# Drop Unnamed columns\n",
        "data = data.drop(unnamed_columns, axis=1)\n",
        "\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpAhfBuqicL_",
        "outputId": "33d60edc-dbee-4ce7-eb64-135c1a4e881c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Label                                              Tweet\n",
            "0  Non_HS  Fadli Zon Minta Mendagri Segera Menonaktifkan ...\n",
            "1  Non_HS  Mereka terus melukai aksi dalam rangka memenja...\n",
            "2  Non_HS  bagaimana gurbernur melakukan kekerasan peremp...\n",
            "3  Non_HS  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
            "4  Non_HS                Waspada KTP palsu.....kawal PILKADA\n",
            "\n",
            "\n",
            "     Label                                       Tweet\n",
            "2703    HS                          Dasar murahan kamu\n",
            "2704    HS     kuliah aja tinggi, tapi otak di dengkul\n",
            "2705    HS                    semoga anda masuk neraka\n",
            "2706    HS                               Matilo anjing\n",
            "2707    HS  Orang timur kurang pintar dari orang barat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqHwNz8Miiz6",
        "outputId": "3246d09a-1323-43eb-9aef-bbaab09c9d7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Non_HS    1354\n",
              "HS        1354\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(data):\n",
        "    labels = []\n",
        "    tweets = []\n",
        "    \n",
        "    for _, row in data.iterrows():\n",
        "        cond = (0 if row['Label'] == \"HS\" else 1)\n",
        "        labels.append(cond)\n",
        "        tweets.append(row['Tweet'])\n",
        "            \n",
        "    return labels, tweets\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "labels, tweets = parse_data(data)"
      ],
      "metadata": {
        "id": "BIUMesqJjnYU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Example number in dataset is {len(tweets)} examples\\n\")\n",
        "\n",
        "print(f\"2nd example:\\n{tweets[1]}\\n\")\n",
        "print(f\"Last example:\\n{tweets[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq6tsGbFjvMZ",
        "outputId": "eeb61817-d707-4ddd-8b6e-721871702ac7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example number in dataset is 2708 examples\n",
            "\n",
            "2nd example:\n",
            "Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.\n",
            "\n",
            "Last example:\n",
            "Orang timur kurang pintar dari orang barat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#90-10 ratio train-test\n",
        "def train_test_split(labels,tweets):\n",
        "    train_size = int(len(tweets) * 0.9)\n",
        "\n",
        "    train_labels = labels[:train_size]\n",
        "    train_tweets = tweets[:train_size]\n",
        "\n",
        "    test_labels = labels[train_size:]\n",
        "    test_tweets = tweets[train_size:]\n",
        "    \n",
        "    return train_labels, train_tweets, test_labels, test_tweets"
      ],
      "metadata": {
        "id": "szc3TGZBj71A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels, train_tweets, test_labels, test_tweets = train_test_split(labels, tweets)\n",
        "\n",
        "print(f\" {len(train_labels)} sentences for training.\")\n",
        "print(f\" {len(train_tweets)} labels for training.\")\n",
        "print(f\" {len(test_labels)} sentences for validation.\")\n",
        "print(f\" {len(test_tweets)} labels for validation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSLA9EVIj-M_",
        "outputId": "e8dd34bf-7893-4791-9169-2241b26138cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2437 sentences for training.\n",
            " 2437 labels for training.\n",
            " 271 sentences for validation.\n",
            " 271 labels for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tweets[:3])\n",
        "print(train_labels[:3])\n",
        "print(test_tweets[:3])\n",
        "print(test_labels[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhGc3XwgkJpO",
        "outputId": "0b7dff00-1fa1-4325-9b4c-074b22d8642c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fadli Zon Minta Mendagri Segera Menonaktifkan Ahok Jadi Gubernur DKI', 'Mereka terus melukai aksi dalam rangka memenjarakan Ahok atau Ahok gagal dalam Pilkada.', 'bagaimana gurbernur melakukan kekerasan perempuan? Buktinya banyak ibu2 mau foto bareng']\n",
            "[1, 1, 1]\n",
            "['gara gara ini negara kita diketawain negara sebelah aduuuh emang iq yang buat tinggi2 yaa ngga kuat gue', 'kasian para bapak bangsa yang merumuskan pancasila mereka pasti sedih klo tau ternyata generasi pancasila nya spt itu', 'mereka gak kaya seperti lu yang gak pernah susah dapat makan']\n",
            "[0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer function\n",
        "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
        "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "bWXZGhsYkL48"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#buat test doang\n",
        "def lowercase(list_sentence):\n",
        "    lower_sentence = list_sentence\n",
        "    for i in range(len(list_sentence)):\n",
        "        lower_sentence[i] = lower_sentence[i].lower()\n",
        "    return lower_sentence"
      ],
      "metadata": {
        "id": "iFo41zctkRiu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize sentence\n",
        "#test_tweets1 = test_tweets[0].lower()\n",
        "#test_tweets1 = test_tweets1.lower()\n",
        "lower_train_tweets = lowercase(train_tweets)\n",
        "tokenizer = fit_tokenizer(lower_train_tweets, NUM_WORDS, OOV_TOKEN)\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK4DKQfkkZpw",
        "outputId": "3afa2d7b-d973-4c06-d95f-5233995bafd6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 7505 words\n",
            "\n",
            "<OOV> token included in vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seq and padding function\n",
        "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen = maxlen, padding = padding)\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "SvbQ_erJkghY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq and padding sentence\n",
        "lower_val_tweets = lowercase(test_tweets)\n",
        "train_padded_seq = seq_and_pad(lower_train_tweets, tokenizer, PADDING, MAXLEN)\n",
        "val_padded_seq = seq_and_pad(lower_val_tweets, tokenizer, PADDING, MAXLEN)\n",
        "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
        "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXcSFp26kkbq",
        "outputId": "747f9917-cb11-4e44-e0cf-b2b82b3c4cd1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded training sequences have shape: (2437, 120)\n",
            "\n",
            "Padded validation sequences have shape: (271, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Structure\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length = MAXLEN),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(units = 32, activation= 'relu'),\n",
        "    tf.keras.layers.Dense(units = 2, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "o4GpRdLukpfJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GpqsSc1kvXu",
        "outputId": "e9922615-03ce-4a95-ba02-1566f98bd8d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_changed = np.array(test_labels)\n",
        "print(type(test_labels_changed))\n",
        "print(type(train_padded_seq))\n",
        "print(type(val_padded_seq))\n",
        "print(type(train_labels))\n",
        "print(\"\\n\")\n",
        "print(train_padded_seq)\n",
        "print(\"\\n\")\n",
        "print(val_padded_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE0_1Cynky9o",
        "outputId": "9a13109c-92cf-49a0-a3db-f1cb6f7ae69b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'list'>\n",
            "\n",
            "\n",
            "[[  1   1 263 ...   0   0   0]\n",
            " [ 61  85   1 ...   0   0   0]\n",
            " [458   1 372 ...   0   0   0]\n",
            " ...\n",
            " [  1   1   1 ...   0   0   0]\n",
            " [ 82  10   1 ...   0   0   0]\n",
            " [218   6 749 ...   0   0   0]]\n",
            "\n",
            "\n",
            "[[624 624   8 ...   0   0   0]\n",
            " [  1 208  70 ...   0   0   0]\n",
            " [ 61  33 163 ...   0   0   0]\n",
            " ...\n",
            " [ 84 117 300 ...   0   0   0]\n",
            " [  1  92   0 ...   0   0   0]\n",
            " [ 12   1 935 ...   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_changed = np.array(train_labels)\n",
        "print(train_labels_changed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ablMChEqmbiR",
        "outputId": "67579d37-8b90-4622-bc5b-adaff8f049ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan LabelEncoder untuk mengubah label menjadi bilangan bulat\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels_changed)\n",
        "test_labels_encoded = label_encoder.transform(test_labels_changed)\n",
        "\n",
        "# Mengubah label menjadi one-hot encoded\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels_encoded, num_classes=num_classes)\n",
        "test_labels_one_hot = tf.keras.utils.to_categorical(test_labels_encoded, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "bCkmTQVjqm91"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels_encoded)\n",
        "print(\"\\n\")\n",
        "print(test_labels_encoded)\n",
        "print(\"\\n\")\n",
        "print(train_labels_one_hot)\n",
        "print(\"\\n\")\n",
        "print(test_labels_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQBWa9AjxjKg",
        "outputId": "ff3fe024-81ed-4bc2-9d1a-d8fc3c75d6df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "\n",
            "\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam() ,metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_padded_seq, train_labels_one_hot, epochs=30, validation_data=(val_padded_seq, test_labels_one_hot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDQB1sGXpJ3m",
        "outputId": "11d60bde-4fc7-4e51-e43d-743cdf25729f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "77/77 [==============================] - 2s 11ms/step - loss: 0.6893 - accuracy: 0.5531 - val_loss: 0.7802 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5556 - val_loss: 0.8120 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 0.6817 - accuracy: 0.5556 - val_loss: 0.8275 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.6735 - accuracy: 0.5560 - val_loss: 0.7780 - val_accuracy: 0.0111\n",
            "Epoch 5/30\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.6533 - accuracy: 0.5815 - val_loss: 0.8242 - val_accuracy: 0.0221\n",
            "Epoch 6/30\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 0.6059 - accuracy: 0.6951 - val_loss: 0.8597 - val_accuracy: 0.1402\n",
            "Epoch 7/30\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.5282 - accuracy: 0.8223 - val_loss: 0.7745 - val_accuracy: 0.4982\n",
            "Epoch 8/30\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.8646 - val_loss: 0.8189 - val_accuracy: 0.5277\n",
            "Epoch 9/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8740 - val_loss: 1.1250 - val_accuracy: 0.3026\n",
            "Epoch 10/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8798 - val_loss: 0.9760 - val_accuracy: 0.4945\n",
            "Epoch 11/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8958 - val_loss: 1.0834 - val_accuracy: 0.4539\n",
            "Epoch 12/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.9011 - val_loss: 0.8584 - val_accuracy: 0.6236\n",
            "Epoch 13/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.9032 - val_loss: 0.8708 - val_accuracy: 0.6273\n",
            "Epoch 14/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.9110 - val_loss: 0.8737 - val_accuracy: 0.6347\n",
            "Epoch 15/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9151 - val_loss: 0.8768 - val_accuracy: 0.6384\n",
            "Epoch 16/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9224 - val_loss: 1.0106 - val_accuracy: 0.5793\n",
            "Epoch 17/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.9261 - val_loss: 1.0181 - val_accuracy: 0.5793\n",
            "Epoch 18/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9282 - val_loss: 1.0722 - val_accuracy: 0.5498\n",
            "Epoch 19/30\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.2064 - accuracy: 0.9294 - val_loss: 1.0101 - val_accuracy: 0.5941\n",
            "Epoch 20/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9356 - val_loss: 1.0181 - val_accuracy: 0.5904\n",
            "Epoch 21/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9352 - val_loss: 1.1276 - val_accuracy: 0.5351\n",
            "Epoch 22/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.9311 - val_loss: 1.0429 - val_accuracy: 0.5830\n",
            "Epoch 23/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9393 - val_loss: 1.1312 - val_accuracy: 0.5461\n",
            "Epoch 24/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9397 - val_loss: 0.9472 - val_accuracy: 0.6236\n",
            "Epoch 25/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9389 - val_loss: 0.9776 - val_accuracy: 0.6162\n",
            "Epoch 26/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9417 - val_loss: 1.2265 - val_accuracy: 0.5203\n",
            "Epoch 27/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1565 - accuracy: 0.9471 - val_loss: 1.0426 - val_accuracy: 0.5867\n",
            "Epoch 28/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9491 - val_loss: 1.1288 - val_accuracy: 0.5720\n",
            "Epoch 29/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9532 - val_loss: 1.0807 - val_accuracy: 0.5830\n",
            "Epoch 30/30\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9520 - val_loss: 0.9127 - val_accuracy: 0.6642\n"
          ]
        }
      ]
    }
  ]
}